{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de2c7de-ded0-4a23-8795-40e43a14d3ba",
   "metadata": {},
   "source": [
    "# Federated Learning and Data Distribution\n",
    "\n",
    "The goal of this notebook is to study the impact of the data distribution between clients when training a model in a distributed fashion.\n",
    "\n",
    "We will first create the different scenarios of data distributions. Then, we will perform FL on all these scenarios using a small CNN. Finally we will analyse the differences between them and explore solutions to mitigate data heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68b32e3-fdaa-453b-b1f8-9589ec2f80ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524977e8-dc6a-4f5a-b8eb-a220e6c0a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from utils.tools import plot_example_images, plot_distribution\n",
    "\n",
    "import numpy as np\n",
    "import syft as sy\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850fed68-be27-4b47-917f-7646d84f5bae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e590206-1004-42b6-9fe0-5cb736bf6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root=\"data\", train=True, transform=ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf59cee-c1b4-45db-9672-237cf5a745c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Explore Train and Test data\n",
    "\n",
    "- What is the structure of the datasets `train_data` and `test_data`?\n",
    "- How many elements is there in the train set? The test set? \n",
    "- Can you find the labels? And the images (matrices)?\n",
    "- What is the shape of an image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb5ef25-451f-4c8f-9c23-66efaf58c25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate train labels and features into distinct vectors\n",
    "\n",
    "\"\"\"\n",
    "Put here your code to explore the data\n",
    "\"\"\"\n",
    "train_size = pass\n",
    "test_size = pass\n",
    "train_labels = pass\n",
    "train_features = pass\n",
    "image_shape = pass\n",
    "\n",
    "assert len(train_labels) == len(train_features) == len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5590320-a78a-4eaf-8c88-89bd4bab9dff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization of MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43007f58-dab9-4c20-be51-08f45c91867e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example_images(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c3a9c-9278-4d09-ba6a-291890e1e4fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data distribution among clients: study of different scenarios\n",
    "\n",
    "- Case 1: IID and balanced dataset\n",
    "- Case 2: unbalanced dataset\n",
    "- Case 3: unbalanced dataset + unbalanced class distribution\n",
    "- Case 4: extrem asymmetry, no class overlap among clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0d08e7-7782-421d-8701-e9e35575ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clients = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad5f27-522f-423a-98d0-453f8056bc40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Case 1: IID and balanced dataset\n",
    "\n",
    "First, let's create `n_clients` datasets that are IDD and balanced. This means that all datasets have the same proportion of images of all classes, and that all datasets have the same size.\n",
    "\n",
    "_Help_: check `random_split` method from pytorch `torch.utils.data`, that randomly split a dataset into non-overlapping new datasets of given lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a16503-a5be-437c-ae76-9ffb687bbf27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_balanced_dataset(train_data: Dataset, n_clients: int) -> List[Dataset]:\\n    \"\"\"\\n    TODO\\n    This function should return a list of training datasets that are balanced\\n    \"\"\"\\n    pass\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_balanced_dataset(train_data: Dataset, n_clients: int) -> List[Dataset]:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    This function should return a list of training datasets that are balanced\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fb6296-2a65-4ff0-a70f-4a28d42f5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of client datasets\n",
    "balanced_datasets = create_balanced_dataset(train_data, n_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052e7d4-7821-4023-a015-1266903bb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "plot_distribution(balanced_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123857d-3b36-44fb-89af-a9d061ed0abe",
   "metadata": {},
   "source": [
    "### Case 2: Unbalanced dataset\n",
    "\n",
    "Now, let's create `n_clients` datasets that are IDD but unbalanced. This means that all datasets have the same proportion of images of all classes, **but** now some datasets are much bigger or smaller than others.\n",
    "\n",
    "_Help_: you can reuse `random_split` method with different arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36288a35-ba91-4867-abcd-bbea2c92f9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_unbalanced_dataset(train_data: Dataset, client_percentages: List[int]):\\n    \"\"\"\\n    TODO\\n    This function should return a list of training datasets that are unbalanced\\n    \"\"\"\\n    pass\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_unbalanced_dataset(train_data: Dataset, client_percentages: List[int]):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    This function should return a list of training datasets that are unbalanced\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6ccf6-a160-4d10-818e-02b053674821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of the MNIST dataset to distribute to each client\n",
    "client_percentages = [5, 35, 60]\n",
    "\n",
    "# Create a list of client datasets\n",
    "unbalanced_datasets = create_unbalanced_dataset(train_data, client_percentages)\n",
    "\n",
    "print(f\"Number of data for client 1 : {len(unbalanced_datasets[0])}\")\n",
    "print(f\"Number of data for client {n_clients} : {len(unbalanced_datasets[-1])}\")\n",
    "\n",
    "assert len(unbalanced_datasets[0]) == client_percentages[0] * len(train_data) // 100\n",
    "assert len(unbalanced_datasets[-1]) == client_percentages[-1] * len(train_data) // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1115de-19b6-4406-b922-412d5cf1e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(unbalanced_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3ecd1-686a-482c-9578-896c726b4627",
   "metadata": {},
   "source": [
    "### Case 3: unbalanced class distribution\n",
    "\n",
    "This time, we'll generate `n_clients` datasets that have unbalanced class distributions: all of them should have the same size **but** not the same class proportions.\n",
    "\n",
    "To be more precise, as we have 3 workers, you can try to have more `0`s, `1`s and `2`s on the first worker, more `3`s, `4`s, `5`s and `6`s on the second and more `7`s,  `8`s and `9`s on the third. If you do this, when plotting the distributions, you should get something similar to the following graph:\n",
    "\n",
    "![non iid](images/non-iid-distribution.png \"non-iid\")\n",
    "\n",
    "_Help_: \n",
    "- Order the data by labels\n",
    "- Use the `Subset` method from pytorch `torch.utils.data`, to create each client dataset separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fabe80-f527-4fda-a70f-81376eb007a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noniid_dataset(train_data: Dataset, *args):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    This function should return a list of training datasets that are not iid\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4850a-b7e8-40a4-bc65-a794e9309de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "noniid_datasets = create_noniid_dataset(train_data, n_clients, split_nb = 2)\n",
    "\n",
    "for i in range(n_clients):\n",
    "    print(f\"Number of data for client {i+1} : {len(noniid_datasets[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632d16f-d878-4803-a3a7-233fdf48831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(noniid_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389dd7d6-61a5-4522-9917-f200f8a2f11d",
   "metadata": {},
   "source": [
    "### Case 4 : extreme asymmetry\n",
    "\n",
    "Here, we'll try to break FL by creating datasets for which the learning process will be very hard (or will not work at all!). \n",
    "\n",
    "We won't give any more information on how to do this, try to use what you've learnt. The only constraints is that each worker should have at least 5000 data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473f180-57c9-40ab-8138-feb82fce1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extreme_dataset(*args):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    This function should return a list of training datasets that FL will be very bad at learning on\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c40b0-7611-4768-a22b-91765156ef0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extreme_datasets = create_extreme_dataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89496d67-7e89-4383-b9d5-2f2ee68e4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(extreme_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21c5cb-a59f-4095-b2bc-6da7e5afbcb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Let's train a model on these data\n",
    "\n",
    "In this part, you'll have to implement a FL training procedure using a basic Federated Averaging algorithm.\n",
    "\n",
    "This notebook plays the role of the aggregator so the \"learning steps\" will be done on remote workers. We'll use PySyft's `VirtualMachine` abstraction.\n",
    "\n",
    "## Quick introduction to PySyft\n",
    "\n",
    "PySyft is a python library designed to enable manipulation of remote data that you cannot see. You can learn more about it [here](https://github.com/OpenMined/PySyft/tree/syft_0.5.0) (note that we'll use the version 0.5.0 in this notebook).\n",
    "\n",
    "As we try to understand what happend during federated learning under the hood, we'll only use low-level abstractions and functions of this library.\n",
    "\n",
    "Basically, the only feature we'll use are:\n",
    "- Sending local objects to a remote worker: PySyft makes it easy to send almost every object you usually for ML with pytorch. For instance, you can do\n",
    "\n",
    "```\n",
    "worker = sy.VirtualMachine(name=\"worker\")\n",
    "\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5, 6])\n",
    "\n",
    "ptr_a = a.send(worker)\n",
    "ptr_b = b.send(worker)\n",
    "```\n",
    "\n",
    "- Manipulating data on a remote worker: once you've sent a object to a worker, you get a pointer to this object (try to run the code given just above and print ptr_a). This pointer is designed to make you manipulate the object it points at seemlessly: use it as you would use a local object in any operation:\n",
    "\n",
    "```\n",
    "ptr_res = a + b\n",
    "```\n",
    "    as the data was remote, what is returned is a pointer to the result.\n",
    "    \n",
    "- Retrieving data from a remote worker: you can use the `get` method of a pointer to retrieve the underlying data:\n",
    "\n",
    "```\n",
    "local_res = ptr_res.get()\n",
    "```\n",
    "\n",
    "### Useful PySyft methods \n",
    "\n",
    "- `.send(worker_client)`: you can use this to send an object to another worker. See how it's done in the `# Dispatch data` cell for an example.\n",
    "- `.get()`: you can use this to retrieve the data a pointer points to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e242c9-9ec5-487f-8a75-11319986f0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the clients that will take part in the federated learning process\n",
    "alice = sy.VirtualMachine(name=\"alice\")\n",
    "bob = sy.VirtualMachine(name=\"bob\")\n",
    "charlie = sy.VirtualMachine(name=\"charlie\")\n",
    "\n",
    "alice_client = alice.get_root_client()\n",
    "bob_client = bob.get_root_client()\n",
    "charlie_client = charlie.get_root_client()\n",
    "\n",
    "workers = [alice_client, bob_client, charlie_client]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc7bb1cc-2d58-4243-aec3-ccd929b62aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't ask you to design an NN architecture, instead here is the model\n",
    "# In order to use PySyft, we'll have to use the following wrapper\n",
    "class SyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.conv1 = self.torch_ref.nn.Conv2d(1, 32, 3, 3)\n",
    "        self.conv2 = self.torch_ref.nn.Conv2d(32, 64, 3, 3)\n",
    "        self.relu = self.torch_ref.nn.ReLU()\n",
    "        self.max_pool2d = self.torch_ref.nn.MaxPool2d(2, 1)\n",
    "        self.flatten = self.torch_ref.nn.Flatten(1)\n",
    "        self.lin1 = self.torch_ref.nn.Linear(64, 32)\n",
    "        self.lin2 = self.torch_ref.nn.Linear(32, 10)\n",
    "        self.log_softmax = self.torch_ref.nn.LogSoftmax(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.lin1(x))\n",
    "        x = self.log_softmax(self.lin2(x))\n",
    "        return x\n",
    "\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fa0158f-2c58-4b6f-bca4-0c57084068c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads_on_worker(local_model, worker_client, worker_inputs, worker_targets):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \n",
    "    In this function you'll compute the gradient for a worker.\n",
    "    To do so:\n",
    "    - first send the model to the worker using PySyft's functions\n",
    "    - use the model on the worker's data\n",
    "    - compute the loss (using worker_client.torch.nn.functional.cross_entropy)\n",
    "        and apply torch's `backward` method on it\n",
    "    - get the parameters' gradients and return them: a model has a `.parameters` method that\n",
    "        returns a list of parameters and each parameter has a `.grad` attribute\n",
    "    \"\"\"\n",
    "    # Send local model to workers\n",
    "    \n",
    "    # Use the model on the worker's data\n",
    "\n",
    "    # Compute loss\n",
    "    \n",
    "    # Get gradients\n",
    "    \n",
    "    pass\n",
    "\n",
    "def average_remote_gradients(workers_gradients):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \n",
    "    In this function you'll compute the mean of the worker gradients.\n",
    "    To do so:\n",
    "    - fetch the gradients using PySyft's `get` method.\n",
    "    - average them (you can use numpy to do so)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def update_local_model(local_model, opt_local, average_grads):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \n",
    "    In this function you'll compute the new global model\n",
    "    To do so:\n",
    "    - assign the average gradient you computed to the global model\n",
    "    - use PyTorch's `step()` method on the optimizer\n",
    "    \"\"\"\n",
    "    # Don't forget to zero out the optimizer state\n",
    "    opt_local.zero_grad()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a2cf24c-99a7-469a-8fdf-d912665a481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also provide you with the following evaluation function\n",
    "def evaluate(model, eval_input, eval_target):\n",
    "    eval_pred = model(eval_input)\n",
    "    eval_acc = torchmetrics.functional.accuracy(eval_pred, eval_target)\n",
    "\n",
    "    return eval_acc\n",
    "\n",
    "def train(local_model, workers, data_pointers, target_pointers, n_iter, batch_size, len_workers_data):\n",
    "    workers_gradients = []\n",
    "    for worker_client, worker_data, worker_targets, len_data in zip(workers, data_pointers, target_pointers, len_workers_data):\n",
    "        slice_start = (n_iter * batch_size) // len_data\n",
    "        slice_start = slice_start if slice_start + batch_size < len_data else 0\n",
    "        workers_gradients.append(\n",
    "            compute_grads_on_worker(\n",
    "                local_model,\n",
    "                worker_client,\n",
    "                worker_data[slice_start:slice_start+batch_size],\n",
    "                worker_targets[slice_start:slice_start+batch_size],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # Average gradients locally\n",
    "    average_grads = average_remote_gradients(workers_gradients)\n",
    "\n",
    "    # Update local model\n",
    "    update_local_model(local_model, opt_local, average_grads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f2eec04-2d4b-4179-a02d-da0d8b8d5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set \n",
    "# Here we keep the test set on the server side, it is the same for all and do not have to be distributed.\n",
    "# Usually, in Federated Learning, a validation set can be sent with the model to the clients to compute metrics.\n",
    "\n",
    "eval_data = datasets.MNIST(root=\"data\", train=False, transform=ToTensor())\n",
    "eval_features, eval_labels = convert_dataset_to_tensors(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e055d336-ad22-4679-bf67-5af50172cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatch_data(workers, clients_features, clients_labels):\n",
    "    data_pointers = []\n",
    "    target_pointers = []\n",
    "    for worker, client_features, client_labels in zip(workers, clients_features, clients_labels):\n",
    "        data_pointers.append(client_features.send(worker))\n",
    "        target_pointers.append(client_labels.send(worker))\n",
    "        \n",
    "    return data_pointers, target_pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70b5e8-8a3a-4c71-88d2-878e367e172d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Case 1: IID and balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63d95102-0ea5-4b1a-b871-b7653616f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here are the data and labels that each of these clients have on their side.\n",
    "# Note that in a real case, you wouldn't see them but they would directly come from the client side.\n",
    "\n",
    "clients_features, clients_labels = convert_datasets_to_tensors(balanced_datasets)\n",
    "len_workers_data = [len(data) for data in clients_labels]\n",
    "# Dispatch data\n",
    "data_pointers, target_pointers = dispatch_data(workers, clients_features, clients_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8636c0-ce0b-4010-8563-3dba98f36fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "local_model = SyNet(torch)\n",
    "opt_local = optim.Adam(params=local_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for n_iter in range(10):\n",
    "    train(local_model, workers, data_pointers, target_pointers, n_iter, batch_size, len_workers_data)\n",
    "\n",
    "    # Compute local loss to monitor training\n",
    "    if n_iter % 5 == 0:\n",
    "        eval_acc = evaluate(local_model, eval_features, eval_labels)\n",
    "        print(eval_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bed4d4-dedb-4234-a102-627a732d7a62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Case 2: Unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be5508bb-83e9-4a86-8278-9af48f96f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_features, clients_labels = convert_datasets_to_tensors(unbalanced_datasets)\n",
    "len_workers_data = [len(data) for data in clients_labels]\n",
    "# Dispatch data\n",
    "data_pointers, target_pointers = dispatch_data(workers, clients_features, clients_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e605c5-391b-46ca-bcfc-a07d56c16529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "local_model = SyNet(torch)\n",
    "opt_local = optim.Adam(params=local_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for n_iter in range(10):\n",
    "    train(local_model, workers, data_pointers, target_pointers, n_iter, batch_size, len_workers_data)\n",
    "\n",
    "    # Compute local loss to monitor training\n",
    "    if n_iter % 5 == 0:\n",
    "        eval_acc = evaluate(local_model, eval_features, eval_labels)\n",
    "        print(eval_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e877cc-11c4-411c-b7da-5d4d15d3eca4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Case 3: Non-iid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff9850e5-4577-47ca-be5f-c2e43717a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_features, clients_labels = convert_datasets_to_tensors(noniid_datasets)\n",
    "len_workers_data = [len(data) for data in clients_labels]\n",
    "\n",
    "noniid_dataset_tensors = list(zip(clients_features, clients_labels ))\n",
    "random.shuffle(noniid_dataset_tensors)\n",
    "clients_features, clients_labels  = list(zip(*noniid_dataset_tensors))\n",
    "\n",
    "# Dispatch data\n",
    "data_pointers, target_pointers = dispatch_data(workers, clients_features, clients_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8137d-1483-4a88-90ea-70ccc1167ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "local_model = SyNet(torch)\n",
    "opt_local = optim.Adam(params=local_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for n_iter in range(10):\n",
    "    train(local_model, workers, data_pointers, target_pointers, n_iter, batch_size, len_workers_data)\n",
    "\n",
    "    # Compute local loss to monitor training\n",
    "    if n_iter % 5 == 0:\n",
    "        eval_acc = evaluate(local_model, eval_features, eval_labels)\n",
    "        print(eval_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8fcf0-258a-43fe-9328-38981b5d68c0",
   "metadata": {},
   "source": [
    "## Case 4: Extreme dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d289971e-732a-48ab-b881-494c00e4c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_features, clients_labels = convert_datasets_to_tensors(extreme_datasets)\n",
    "len_workers_data = [len(data) for data in clients_labels]\n",
    "# Dispatch data\n",
    "data_pointers, target_pointers = dispatch_data(workers, clients_features, clients_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cddea-cd5a-48ab-a5bf-75521b82ca48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "local_model = SyNet(torch)\n",
    "opt_local = optim.Adam(params=local_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for n_iter in range(10):\n",
    "    train(local_model, workers, data_pointers, target_pointers, n_iter, batch_size, len_workers_data)\n",
    "\n",
    "    # Compute local loss to monitor training\n",
    "    if n_iter % 5 == 0:\n",
    "        eval_acc = evaluate(local_model, eval_features, eval_labels)\n",
    "        print(eval_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62732cb2",
   "metadata": {},
   "source": [
    "* You should have obtained an accuracy around 0.10 (10%), what does it mean?\n",
    "* Has your model learned anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8364ec-d92b-4acc-9195-e1601417c475",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyses\n",
    "\n",
    "Let's try to summarize what happened in all the experiments we did!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a619db-94a5-43be-b1f6-4795cf9ab745",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Final accuracy\n",
    "\n",
    "The first thing we want to know is how well the final model performs. To do so, print the accuracy of each model (models that have been trained using the different data distribution) and compare them.\n",
    "\n",
    "What do you think about the results you got? Is it what you expected? Why do you think did this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22620ae2-46e7-40d3-a6d9-cdfe87739e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb5a32-b2bd-4d45-9310-4e53e0545e77",
   "metadata": {},
   "source": [
    "## Convergence rate\n",
    "\n",
    "Now, we'll try to see how fast these performances were reached. Try to plot a graph \n",
    "\n",
    "To do so, you may have to tweak a bit the training functions that are above (to keep track of the evaluation accuracy during the training) and re-train everything... But as we work with toy data, this shoudln't take too long.\n",
    "\n",
    "What do you think about the results you got? Is it what you expected? Why do you think did this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832d58e-7ede-4c7f-8346-4b93d9603472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251b89e-e3d4-4d9a-9bee-7c440d7ae7b2",
   "metadata": {},
   "source": [
    "# Compensate non-IID data \n",
    "\n",
    "One strategy is to compensate non-IID data by sending a validated dataset from the server to each clients.\n",
    "\n",
    "_Help_: \n",
    "- Put aside a small random subset of `3000` samples from `train_data` that won't be distributed among clients (at first).\n",
    "- Make sure the small dataset is IID (same number of samples per class).\n",
    "- Create client datasets on the rest of the `train_data` set (containing `len(train_data) - 3000` samples), following the extrem iid-case.\n",
    "- Add the kept dataset to each client datasets (check `ConcatDataset` from pytorch `torch.utils.data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397ae67-7431-4714-a9f7-f688bd3aff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compensated_dataset(train_data, n_data_kept, client_percentages):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4da2ac-4d30-469e-9744-81e2862c1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_kept = 3000\n",
    "compensated_datasets = create_compensated_dataset(train_data, n_data_kept, client_percentages)\n",
    "\n",
    "print(f\"Number of data for client 1 : {len(client_datasets[0])}\")\n",
    "print(f\"Number of data for client {n_clients} : {len(client_datasets[-1])}\")\n",
    "\n",
    "assert len(compensated_datasets[0]) == (len(train_data)-n_data_kept) * client_percentages[0] // 100 + n_data_kept\n",
    "assert len(compensated_datasets[-1]) == (len(train_data)-n_data_kept) * client_percentages[-1] // 100 + n_data_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4ea99-9142-40ea-b37d-c119af21480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(client_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37eb03b-fdff-4496-91f2-8b8c82a9b709",
   "metadata": {},
   "source": [
    "## Model training on this compensated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31278d8-9ca1-454d-b327-144fe04607a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clients_features, clients_labels = convert_datasets_to_tensors(compensated_datasets)\n",
    "len_workers_data = [len(data) for data in clients_labels]\n",
    "# Dispatch data\n",
    "data_pointers, target_pointers = dispatch_data(workers, clients_features, clients_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7e203-870f-41a0-bf96-0558e36ca303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "for epoch in range(3):\n",
    "    for n_iter in range(len_worker_data // batch_size):\n",
    "        train(local_model, workers, data_pointers, target_pointers, n_iter, batch_size)\n",
    "\n",
    "        # Compute local loss to monitor training\n",
    "        if n_iter % 20 == 0:\n",
    "            eval_acc = evaluate(local_model, (eval_features, eval_labels))\n",
    "            print(eval_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13264797-4b20-461a-93b8-8a3bbf1cee2e",
   "metadata": {},
   "source": [
    "## Other strategies to compensate non-IID data\n",
    "\n",
    "- What are the drawbacks of the method you have just implemented?\n",
    "- Can you explain three other ways to compensate non-IID data? _(no code required)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ed56c-98a5-4525-a6d3-b77dc8543aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
